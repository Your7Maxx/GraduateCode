{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc3fc1cd-fad8-4e2a-a08e-282c62b48d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/commandDetect/randomForest/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 2.7112604777018228\n",
      "Epoch 2 - Average Loss: 2.5004400147332086\n",
      "Epoch 3 - Average Loss: 2.3516721460554333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33         5\n",
      "           1       0.40      1.00      0.57         8\n",
      "           2       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         1\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         1\n",
      "          12       0.00      0.00      0.00         1\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.00      0.00      0.00         5\n",
      "          16       0.14      1.00      0.25         2\n",
      "\n",
      "    accuracy                           0.31        35\n",
      "   macro avg       0.13      0.18      0.10        35\n",
      "weighted avg       0.24      0.31      0.19        35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# 设置一些超参数\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_SEQ_LENGTH = 512  # 您可以根据您的数据集和模型选择合适的最大序列长度\n",
    "\n",
    "# 定义BERT模型和标记化器\n",
    "BERT_PATH = '/root/commandDetect/randomForest/bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "# 加载数据\n",
    "data_path = '/root/commandDetect/randomForest/222.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['Command', 'Technique'])\n",
    "# 编码标签\n",
    "label_map = {label: idx for idx, label in enumerate(df['Technique'].unique())}\n",
    "df['Technique'] = df['Technique'].map(label_map)\n",
    "NUM_CLASSES = len(label_map)\n",
    "\n",
    "# 创建数据集\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "for text, label in zip(df['Command'], df['Technique']):\n",
    "    # 截断文本以适应最大序列长度\n",
    "    encoded_text = tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length', max_length=MAX_SEQ_LENGTH, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_text['input_ids'])\n",
    "    attention_masks.append(encoded_text['attention_mask'])\n",
    "    labels.append(label)\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 划分训练集、验证集和测试集\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = (len(dataset) - train_size) // 2\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# 定义BERT分类模型\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_PATH, num_labels=NUM_CLASSES)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") 内存不够，咋样分隔都不够\n",
    "device = torch.device(\"cpu\") \n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Average Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        y_pred.extend(torch.argmax(logits, dim=1).tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "# 报告分类性能\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c3f50-f30e-4f97-bdcc-1bf5f231c6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7beb3bcb-6b9a-4d5e-a06d-0524b4563914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nc', '-', 'l', '##v', '##p', '##n', '222', '##3']\n",
      "tensor([[  101,  1045,  2097,  3422,  2033, 23065,  3892,   102,     0,     0]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "BERT_PATH = '/root/commandDetect/randomForest/bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "print(tokenizer.tokenize('nc -lvpn 2223'))\n",
    "\n",
    "example_text = 'I will watch Memento tonight'\n",
    "bert_input = tokenizer(example_text,padding='max_length', \n",
    "                       max_length = 10, \n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\")\n",
    "# ------- bert_input ------\n",
    "print(bert_input['input_ids'])\n",
    "print(bert_input['token_type_ids'])\n",
    "print(bert_input['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05fb27-5df5-46ab-90de-2d44b31ce9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49f33eee-617e-4b8a-b22d-6afe932b1454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenized': [['nc', '-lvnp', '2223']], 'counter': Counter({'nc': 1, '-lvnp': 1, '2223': 1})}\n"
     ]
    }
   ],
   "source": [
    "open('slpp.py', 'w').write(open('/root/commandDetect/xgb/slp/slp2.py', 'r').read())\n",
    "\n",
    "from slpp import ShellTokenizer\n",
    "\n",
    "X={}\n",
    "t = ShellTokenizer(verbose=True)\n",
    "X[\"tokenized\"], X[\"counter\"] = t.tokenize(['nc -lvnp 2223'])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ce67dc8-ac63-407d-985e-be3f207cc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "class CustomShellTokenizer(PreTrainedTokenizerFast):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # 使用您的ShellTokenizer来分词\n",
    "        t = ShellTokenizer(verbose=True)\n",
    "        tokens, _ = t.tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88b5860b-c69e-4205-aa7f-b6f8f067c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CustomShellTokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign '__main__.CustomShellTokenizer' as child module 'word_embeddings' (torch.nn.Module or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m custom_shell_tokenizer \u001b[38;5;241m=\u001b[39m CustomShellTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(BERT_PATH)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 将自定义tokenizer与BERT模型关联\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustom_shell_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mset_output_embeddings(custom_shell_tokenizer)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 使用自定义tokenizer进行分词\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:903\u001b[0m, in \u001b[0;36mBertModel.set_input_embeddings\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_input_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 903\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1733\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as child module \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1734\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Module or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1735\u001b[0m                         )\n\u001b[1;32m   1736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1737\u001b[0m         output \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign '__main__.CustomShellTokenizer' as child module 'word_embeddings' (torch.nn.Module or None expected)"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BertTokenizer, BertModel\n",
    "\n",
    "BERT_PATH = '/root/commandDetect/randomForest/bert-base-uncased'\n",
    "# 加载BERT模型\n",
    "model = BertModel.from_pretrained(BERT_PATH)\n",
    "\n",
    "# 创建自定义ShellTokenizer实例\n",
    "custom_shell_tokenizer = CustomShellTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "# 将自定义tokenizer与BERT模型关联\n",
    "model.set_input_embeddings(custom_shell_tokenizer)\n",
    "model.set_output_embeddings(custom_shell_tokenizer)\n",
    "\n",
    "# 使用自定义tokenizer进行分词\n",
    "text = \"nc -lvnp 2223\"\n",
    "tokens = custom_shell_tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b17192-854d-44d7-a817-93379836ff5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
